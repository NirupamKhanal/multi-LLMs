# multi-LLMs
Multi-LLMs is a research project that fuses embedding vectors from multiple large language models into a unified third-layer abstraction. By integrating and contextualizing these embeddings originating at each unique parent-LLM space, the project builds a cohesive pipeline to exploit learned structures and yield enhanced NLP performance.
